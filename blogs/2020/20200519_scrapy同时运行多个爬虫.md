---
title: Scrapy同时运行多个爬虫
date: 2020-05-19
tags:
 - 爬虫
 - 框架
 - scrapy
categories:
 - 2020
---


多种实现方案:

1. 开启多个命令行，分别执行scrapy cralw xxxx

2. 编写脚本，执行工程下的所有爬虫
``` py
#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
@file: run.py
@time: 2020/05/19 9:49
@desc: None
@Author: Chenzq
@Wechat: *
@contact: czq181020@gmail.com
"""
from scrapy.utils.project import get_project_settings
from scrapy.crawler import CrawlerProcess

def main():
    setting = get_project_settings()
    process = CrawlerProcess(setting)
    didntWorkSpider = ['tb', 'jd', 'anjuke', '51paimaifang', 'manjuke', 'upd_status']
    print(didntWorkSpider)
    for spider_name in process.spiders.list():
        if spider_name in didntWorkSpider:
            continue
        print("Running spider %s" % (spider_name))
        process.crawl(spider_name)
    process.start()

main()
```

*脚本解析：* 正常情况下，在终端可以通过`scrapy crawl ***` 来运行某个 spider。
过程大致是，首先读取配置，其中 SpiderLoader 会默认读取配置文件中的 SPIDER_MODULES 对应的所有spider，另外还需要初始化各种类，其中包含了 CrawlerProcess 这个重要的类 `cmd.crawler_process = CrawlerProcess(settings)`，然后调用 crawl `scrapy.commands.crawl`, 这个终端命令 run 方法的具体实现，该实现中`self.crawler_process.crawl(spname, **opts.spargs)`，`self.crawler_process.start()`

3. 使用scrapyd，部署爬虫，通过scrapyd的API调用爬虫

4. 推荐使用 [spiderkeeper](https://github.com/DormyMo/SpiderKeeper)或者 [gerapy](https://github.com/Gerapy/Gerapy)，spiderkeeper可以定时运行爬虫。